{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LogReg.ipynb","provenance":[],"mount_file_id":"1KbtaP4CNiKqj7EBtkcH7nKCfG5XgdpEg","authorship_tag":"ABX9TyMNDqqd3sfFznhpSIOaMACi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"U10NaJ1Ng-RC"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import librosa\n","import soundfile as sf\n","import librosa.display\n","from glob import glob\n","import os\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MeoFOrZhNiY"},"source":["def splitData(X, t, testFraction=0.2, randomize = False):\n","    \"\"\"\n","    Split the data randomly into training and test sets\n","    Use numpy functions only\n","    Inputs:\n","        X: (np array of len Nsamples) input feature vectors\n","        t: (np array of len Nsamples) targets; one hot vectors\n","        testFraction: (float) Nsamples_test = testFraction * Nsamples\n","    Outputs:\n","        X_train: training set\n","        X_test: test set\n","        t_train: training labels\n","        t_test: test labels\n","    \"\"\"\n","    if randomize is False:\n","      tot_samples = np.random.RandomState(seed=42).permutation(len(X))\n","    else:\n","      tot_samples = np.random.permutation(len(X))\n","\n","    X_train = []\n","    X_test = []\n","    t_train = []\n","    t_test = []\n","    \n","    test_samples = max(1,int(len(X)*testFraction))\n","    # print(tot_samples[:test_samples])\n","    # print(tot_samples[test_samples:])\n","\n","    for i in range(test_samples):\n","      X_test.append(X[tot_samples[i]])\n","      t_test.append(t[tot_samples[i]])\n","\n","    for i in range(test_samples,len(X)):\n","      X_train.append(X[tot_samples[i]])\n","      t_train.append(t[tot_samples[i]])\n","    \n","    return np.asarray(X_train), np.asarray(t_train), np.asarray(X_test), np.asarray(t_test)\n","\n","def calc_spec(x):\n","    n_fft = 1024\n","    hop_length = 512\n","    win_length = 1024\n","    X = np.abs(librosa.stft(x, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window='hann'))\n","    X = librosa.power_to_db(X**2,ref=np.max)\n","    return X\n","\n","def audio2spec(x, norm=True):\n","    \n","    '''\n","    Compute Mel-frequency cepstral coefficients (MFCCs)\n","    Inputs:\n","        x: np array of shape (Nsamples,)\n","    Output:\n","        X: (np array) spectrogram sequence\n","    '''\n","    X=[]\n","    for sample in x:\n","      X.append(calc_spec(sample))\n","    if norm is True:\n","      X = (X-np.mean(X))/np.std(X)\n","    return np.asarray(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPy_T57fhY5-"},"source":["def computeCM(y, y_hat):\n","    '''\n","    Compute confusion matrix to evaluate your model\n","    Inputs:\n","        y = labels \n","        y_hat = predicted output\n","    Output:\n","        confusion matrix: confusion matrix\n","    '''\n","    tp=0\n","    tn=0\n","    fp=0\n","    fn=0\n","\n","    for i in range(len(y)):\n","      if y[i][0] == 1 and y_hat[i][0] == 1:\n","        tp+=1\n","      elif y[i][0] == 0 and y_hat[i][0] == 0:\n","        tn+=1\n","      elif y[i][0] == 1 and y_hat[i][0] == 0:\n","        fn+=1\n","      elif y[i][0] == 0 and y_hat[i] == 1:\n","        fp+=1\n","\n","    confusion_matrix = [[tp,fp],[fn,tn]]\n","    return np.asarray(confusion_matrix) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFa4MEUjl8jg"},"source":["!pip install noisereduce"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpMsywQfl41b"},"source":["import librosa\n","import os\n","import noisereduce as nr\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","from collections import defaultdict\n","import soundfile as sf\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bW_kevKlsSh"},"source":["def is_overlap(cstart_time, cend_time, start_time, end_time):\n","  if(cstart_time > end_time):\n","    return False\n","  if(cend_time < start_time):\n","    return False\n","  if(start_time > cstart_time and end_time < cend_time ):\n","    return True\n","  if(start_time > cstart_time):\n","    overlap = start_time - cstart_time\n","    if(overlap > 0.50 * (start_time - cend_time)):\n","      return True\n","    else:\n","      return False\n","  if(end_time < cend_time):\n","    overlap = cend_time - end_time\n","    if(overlap > 0.50 * (start_time - cend_time)):\n","      return True\n","    else:\n","      return False\n","  \n","  return True\n","\n","def one_hot_enc(cl_name):\n","  if(cl_name == 'music'):\n","    return np.array([0,1,0])\n","  elif(cl_name == 'speech'):\n","    return np.array([1,0,0])\n","  else:\n","    return np.array([0,0,1])\n","\n","\n","def load_audio(filename, cls, Fs = 16000):\n","    # Return bins of size of 1000 corresponding to \n","    x, _ = librosa.load(filename, sr=Fs)\n","    x = nr.reduce_noise(x, Fs)\n","    x = np.reshape(x[:K], (-1,SPLIT_SIZE))\n","    labels = np.zeros((N,3))\n","    for index, sub_audios in enumerate(x):\n","      class_assigned = False\n","      for tup in enumerate(cls):\n","        if(is_overlap(tup[1][0], tup[1][1], index*SPLIT_SIZE/K, (index+1)*SPLIT_SIZE/K)):\n","          labels[index] = one_hot_enc(tup[1][2])\n","          class_assigned = True\n","          break\n","      if(class_assigned == False):\n","          labels[index] = one_hot_enc('silence')\n","    \n","    return labels, x\n","\n","\n","def load_data(foldername, Fs=16000):\n","  '''\n","  Inputs: \n","      foldername: (str) foldername\n","      Fs: (int) sampling rate\n","  \n","  Output:\n","      data: np array of data\n","  '''\n","\n","  files = os.listdir(foldername+'/wav')\n","  df = pd.read_csv (foldername+'/labels.csv', usecols=['filename','onset','offset','class'])\n","  print(df.head())\n","  labels_data = df.to_numpy()\n","  data = dict()\n","  labels = defaultdict(list)\n","  \n","  for entry in labels_data:\n","    # print(entry[0])\n","    # print((entry[1],entry[2],entry[3]))\n","    labels[entry[0]].append((entry[1],entry[2],entry[3]))\n","  \n","  labels_list = []\n","  data_list = []\n","\n","  for wav_file in tqdm(files):\n","    ls, audio = load_audio(foldername+\"/wav/\"+wav_file, labels[wav_file[:-4]], Fs)\n","    labels_list.append(ls)\n","    data_list.append(audio)\n","\n","  labels_final = np.array(labels_list)\n","  data_final = np.array(data_list)\n","  labels_final = labels_final.reshape(N*len(files), 3)\n","  data_final = data_final.reshape(N*len(files), SPLIT_SIZE)\n","  # print(labels_final.shape)\n","  # print(data_final.shape)\n","  return labels_final, data_final\n","\n","\n","def reverse_one_hot(ohv):\n","  if(ohv[1] == 1):\n","    return \"music\"\n","  elif(ohv[0] == 1):\n","    return \"speech\"\n","  else:\n","    return \"silence\"\n","\n","def save_data(train_labels, train_audios):\n","  for i in range(len(train_audios)):\n","    print(reverse_one_hot(train_labels[i]))\n","    sf.write('/content/drive/MyDrive/Sem 5/EE603/project/val_set/split_wavs/'+str(i)+'.wav', train_audios[i], 16000)\n","\n","def get_mfcc(train_audios, n_mfcc=20, Fs=16000):\n","    mfccs=[]\n","    for audio in tqdm(train_audios):\n","      mfccs.append(librosa.feature.mfcc(audio, n_mfcc=n_mfcc, sr=Fs, n_fft=512))\n","      # mfccs.append(audio_mfcc)\n","    mfccs = np.array(mfccs)\n","    return mfccs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTOiJLIAltMB"},"source":["SAMPLING_RATE = 16000\n","AUDIO_DURATION = 10 #in seconds\n","FRAME_ACCURACY = 0.99\n","SPLIT_SIZE = 1000\n","MAX_SAMPLES = 0\n","K = int(AUDIO_DURATION*SAMPLING_RATE*FRAME_ACCURACY/SPLIT_SIZE)*SPLIT_SIZE\n","N = int(K/SPLIT_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XUHniXSlvXa","executionInfo":{"status":"ok","timestamp":1636749474409,"user_tz":-330,"elapsed":16607,"user":{"displayName":"Shiven Tripathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzq9zlJtl-Oe29ULdfPQjb7bTMJy0SAXakH_QV5A=s64","userId":"08884573075132776173"}},"outputId":"3a6163cc-61f4-4c89-be6c-ff903ab2555e"},"source":["train_labels, train_audios = load_data('/content/drive/MyDrive/Sem 5/EE603/project/val_set')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  filename   onset  offset   class\n","0     S001  0.7545   1.963  speech\n","1     S001  3.0330   4.365  speech\n","2     S001  5.2850   6.591  speech\n","3     S001  7.6340   9.019  speech\n","4     S002  0.1580   1.060  speech\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:15<00:00,  1.88it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycV0RE9v3t1V","executionInfo":{"status":"ok","timestamp":1636749501937,"user_tz":-330,"elapsed":399,"user":{"displayName":"Shiven Tripathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzq9zlJtl-Oe29ULdfPQjb7bTMJy0SAXakH_QV5A=s64","userId":"08884573075132776173"}},"outputId":"cd950ab8-eb27-45cf-a8ba-cdfa1e1db925"},"source":["print(np.shape(train_labels), type(train_labels))\n","print(np.shape(train_audios), type(train_audios))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(4740, 3) <class 'numpy.ndarray'>\n","(4740, 1000) <class 'numpy.ndarray'>\n"]}]},{"cell_type":"code","metadata":{"id":"EveK9UNHhXXH"},"source":["class Classifier: \n","    '''\n","    Create a linear classifier to classify each frame\n","    '''\n","    def __init__(self,lr=10**-2,epoch=10**4):\n","        self.W = -1\n","        self.b = 0\n","        self.lr = lr\n","        self.epoch = epoch\n","\n","    def softMax(self,z):\n","      z = np.exp(z - np.max(z))\n","      return z/np.sum(z)\n","\n","    def train(self,x_train, y_train):\n","        '''\n","        Train the linear classifier\n","        Inputs:\n","            x_train: training set\n","            y_train: training labels\n","        Output:\n","            None\n","        '''\n","        print(\"Received input data and labels of shapes:\",np.shape(x_train), np.shape(y_train))\n","        n_files = np.shape(x_train)[0]\n","        n_features = np.shape(x_train)[1]\n","        n_frames = np.shape(x_train)[2]\n","        n_classes = np.shape(y_train)[1]\n","        m = n_files\n","\n","        print(n_files, \"n_files\")\n","        print(n_frames, \"n_frames\")\n","        print(n_features, \"n_features\")\n","        print(n_classes, \"n_classes\")\n","\n","        self.W = np.random.RandomState(seed=42).random((n_features, n_classes))\n","        self.b = np.random.RandomState(seed=42).random(n_classes)\n","        print(\"Initialised weights and bias with shapes:\", np.shape(self.W), np.shape(self.b))\n","\n","        X = []\n","        Y = []\n","        losses = []\n","\n","        for i in range(n_files):\n","          for j in range(n_frames):\n","            X.append(x_train[i][:,j])\n","            Y.append(y_train[i])\n","\n","        X = np.asarray(X)\n","        Y = np.asarray(Y)\n","        print(\"final shapes of data fed to training:\",np.shape(X),np.shape(Y))\n","\n","        for i in tqdm(range(self.epoch)):\n","            Z = X@self.W + self.b\n","            Y_hat = self.softMax(Z)\n","\n","            w_grad = (1/m)*np.dot(X.T, (Y_hat - Y)) \n","            b_grad = (1/m)*np.sum(Y_hat - Y)\n","            \n","            self.W -= self.lr*w_grad\n","            self.b -= self.lr*b_grad\n","            \n","            # loss = (-1/m)*np.sum(np.log(Y_hat+1e-9)*Y )\n","            loss = -np.mean(np.log(1e-10+Y_hat[np.arange(len(Y)), np.argmax(Y, axis=1)]))\n","\n","            losses.append(loss)\n","        return losses       \n","\n","    def save_model(self, save_path):\n","        '''\n","        Save the trained model on local disk\n","        Input:\n","            save_path: location at which model is to be saved\n","        Output:\n","            None\n","        '''\n","        \n","        with open(os.path.join(save_path,'W.npy'), 'wb') as f:\n","          np.save(f,self.W)\n","        with open(os.path.join(save_path,'b.npy'), 'wb') as f:\n","          np.save(f,self.b)\n","    \n","    def load_model(self, load_path):\n","        '''\n","        Save the trained model on local disk\n","        Input:\n","            load_path: location from which model is to be loaded\n","        Output:\n","            None\n","        '''\n","\n","        with open(os.path.join(load_path,'W.npy'), 'rb') as f:\n","          self.W = np.load(f)\n","        with open(os.path.join(load_path,'b.npy'), 'rb') as f:\n","          self.b = np.load(f)\n","    \n","    def predict_framewise(self,x_test):\n","        '''\n","        Framewise classification (speech or music)\n","        Input:\n","            x_test: single frame (n_features,1)\n","        Output:\n","            y_pred_framewise = class prediction\n","        '''\n","        z = x_test@self.W + self.b\n","        y_hat = np.argmax(self.softMax(z))\n","        y_hot = np.zeros(np.shape(z)[0])\n","        y_hot[y_hat] = 1\n","        return y_hot\n","\n","    def predict_aggregate(self,y_pred_framewise):\n","        '''\n","        Aggregate frames to give a single class label (music or speech) to the entire audio file\n","        Input:\n","            y_pred_framewise: framewise prediction\n","        Output:\n","            y_hat: frame aggregate (one-hot vectors)\n","        '''\n","        y_pred_framewise = np.asarray(y_pred_framewise)\n","        if y_pred_framewise.ndim > 1:\n","          counts = np.sum(y_pred_framewise, axis=0)\n","        y_hat = np.argmax(counts)\n","        y_hot = np.zeros(np.shape(counts)[0])\n","        y_hot[y_hat] = 1\n","        return y_hot\n","\n","    def predict(self,x_test):\n","      '''\n","      Return one hot encoded classification results for the test set\n","      Input: \n","        x_test: testing set (normalised spectrograms)\n","      Output:\n","        y_hat: list of one hot vectors for classification on x_test\n","      '''\n","      y_hat = []\n","      for f in x_test:\n","        y_pred_framewise = []\n","        for window in range(np.shape(f)[1]):\n","          y_pred_framewise.append(self.predict_framewise(f[:,window]))\n","        y_hat.append(self.predict_aggregate(y_pred_framewise))\n","      return y_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"Ri1VTSEImyc_","executionInfo":{"status":"ok","timestamp":1636750174789,"user_tz":-330,"elapsed":40454,"user":{"displayName":"Shiven Tripathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzq9zlJtl-Oe29ULdfPQjb7bTMJy0SAXakH_QV5A=s64","userId":"08884573075132776173"}},"outputId":"e7839aa1-f234-4b3b-fb94-76376eeb66a6"},"source":["EPOCHS = 10**3\n","LR = 10**-2\n","TESTFRACTION = 0.2\n","\n","# train_audios_mfcc = audio2spec(train_audios)\n","# print(np.shape(train_audios_mfcc), type(train_audios_mfcc))\n","# train_audios_mfcc = np.reshape(train_audios_mfcc, (np.shape(train_audios_mfcc)[0],-1,1))\n","# print(np.shape(train_audios_mfcc), type(train_audios_mfcc))\n","\n","X = train_audios\n","Y = train_labels\n","\n","X_train, t_train, X_test, t_test = splitData(X=X, t=Y, testFraction=TESTFRACTION, randomize=True)\n","print(\"check shapes after train test split\")\n","print(np.shape(X_train), np.shape(t_train))\n","print(np.shape(X_test), np.shape(t_test))\n","\n","# TRAINING \n","X_train = audio2spec(x=X_train, norm=True)\n","X_train = np.reshape(X_train, (np.shape(X_train)[0],-1,1))\n","print(\"training\") \n","model = Classifier(lr=LR,epoch=EPOCHS) \n","hist=model.train(x_train=X_train, y_train=t_train) \n","\n","#PLOTTING \n","plt.plot(np.arange(EPOCHS),hist)\n","plt.title(\"Loss vs Epoch at LR = \"+str(LR))\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","X_test = audio2spec(x=X_test, norm=True)\n","X_test = np.reshape(X_test, (np.shape(X_test)[0],-1,1))\n","y_hat = model.predict(x_test=X_test)\n","\n","# # EVALUATION METRICS \n","# confusion_matrix = computeCM(y=t_test, y_hat=y_hat) \n","# plt.matshow(confusion_matrix, cmap='gray')\n","# plt.title(\"Confusion Matrix\")\n","# plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["check shapes after train test split\n","(3792, 1000) (3792, 3)\n","(948, 1000) (948, 3)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=1000\n","  n_fft, y.shape[-1]\n"]},{"output_type":"stream","name":"stdout","text":["training\n","Received input data and labels of shapes: (3792, 1026, 1) (3792, 3)\n","3792 n_files\n","1 n_frames\n","1026 n_features\n","3 n_classes\n","Initialised weights and bias with shapes: (1026, 3) (3,)\n","final shapes of data fed to training: (3792, 1026) (3792, 3)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:37<00:00, 26.35it/s]\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=1000\n","  n_fft, y.shape[-1]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpx5weCHnPTw","executionInfo":{"status":"ok","timestamp":1636750181648,"user_tz":-330,"elapsed":413,"user":{"displayName":"Shiven Tripathi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzq9zlJtl-Oe29ULdfPQjb7bTMJy0SAXakH_QV5A=s64","userId":"08884573075132776173"}},"outputId":"6de8193a-e216-4178-a35e-38b6ea481d2a"},"source":["test_accuracy = np.mean(np.asarray(y_hat).ravel() == np.asarray(t_test).ravel()) * 100\n","# test_accuracy = classifier.score(X_test, y_test)\n","print('Test accuracy:', test_accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 68.42475386779185\n"]}]},{"cell_type":"code","metadata":{"id":"J5JNJlhEhd1m"},"source":["model.save_model('')\n","!ls\n","\n","model.load_model('')\n","print(model.W)\n","print(model.b)\n","print(np.shape(model.W))\n","print(np.shape(model.b))"],"execution_count":null,"outputs":[]}]}